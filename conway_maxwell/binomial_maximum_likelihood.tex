\documentclass[a4paper,12pt]{article}
\usepackage[utf8x]{inputenc}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{mathrsfs}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage[margin=3cm]{geometry}
\usepackage{times}
\usepackage{graphicx}
\usepackage{enumitem}
\usepackage{fancyhdr}
\usepackage{hyperref}
\usepackage{setspace}
\usepackage{subcaption}
\usepackage{mathtools}

\pagestyle{fancy}
\fancyhf{}
\lhead{Thomas Delaney}
\rhead{Maximum likelihood estimation}
\cfoot{\thepage}

\newtheorem{theorem}{Theorem}
\newtheorem{proposition}{Proposition}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{corollary}{Corollary}[section]
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]

\DeclareMathOperator*{\argmin}{arg\,min} % Jan Hlavacek
\DeclareMathOperator*{\argmax}{arg\,max} % Jan Hlavacek
\newcommand{\boldnabla}{\mbox{\boldmath$\nabla$}} % to be used in mathmode
\newcommand{\cbar}{\overline{\mathbb{C}}}% to be used in mathmode
\newcommand{\diff}[2]{\frac{d #1}{d #2}}% to be used in mathmode
\newcommand{\difff}[2]{\frac{d^2 #1}{d #2^2}}% to be used in mathmode
\newcommand{\pdiff}[2]{\frac{\partial #1}{\partial #2}} % to be used in mathmode
\newcommand{\pdifff}[2]{\frac{\partial^2 #1}{\partial #2^2}}% to be used in mathmode
\newcommand{\upperth}{$^{\mbox{\footnotesize{th}}}$}%to be used in text mode
\newcommand{\vect}[1]{\mathbf{#1}}% to be used in mathmode
\newcommand{\curl}[1]{\boldnabla \times \vect{#1}} % to be used in mathmode
\newcommand{\divr}[1]{\boldnabla \cdot \vect{#1}} %to be used in mathmode
\newcommand{\modu}[1]{\left| #1 \right|} %to be used in mathmode
\newcommand{\brak}[1]{\left( #1 \right)} % to be used in mathmode
\newcommand{\comm}[2]{\left[ #1 , #2 \right]} %to be used in mathmode
\newcommand{\dop}{\vect{d}} %to be used in mathmode
\newcommand{\cov}{\text{cov}} %to be used in mathmode
\newcommand{\var}{\text{var}} %to be used in mathmode
\newcommand{\mb}{\mathbf} %to be used in mathmode
\newcommand{\bs}{\boldsymbol} %to be used in mathmode
% Title Page
\title{How informative are retinal ganglion cells?}
\author{Thomas Delaney 1330432}

\begin{document}

\section{Exponential Family}
If the probability mass/density function of a probability distribution with $R$ parameters $\boldsymbol{\theta}$ can be written in the form 
\begin{align}\label{eq:exponential_family_definition}
  P(x | \boldsymbol{\theta}) = h(x) \exp \left( \boldsymbol{\eta}(\boldsymbol{\theta}) \cdot \mathbf{T}(x) - A(\boldsymbol{\theta}) \right)
\end{align}
where $h(x)$ is any function of $x$, $\eta(\boldsymbol{\theta})$ is a vector of reparametrisations called the \textit{natural parameters} of $P$, $\mathbf{T}(x)$ is a vector containing $R$ transformations of $x$ called the \textit{natural sufficient statistics} of $P$, and $A(\boldsymbol{\theta})$ is a partition function, then $P$ is a member of the exponential family of distributions.

\subsection{Maximum likelihood estimation for the exponential family}
For any exponential family distribution 
\begin{align}
  1 = \int P(x | \boldsymbol{\theta}) dx = \int h(x) \exp \left( \boldsymbol{\eta}(\boldsymbol{\theta}) \cdot \mathbf{T}(x) - A(\boldsymbol{\theta}) \right) dx
\end{align}
because it's a probability distribution. Taking the derivative of both sides with respect to the $r$th natural parameter eventually gives
\begin{align}
  E_P \left[ [\mathbf{T}(x)]_r \right] = \pdiff{A(\boldsymbol{\theta})}{\eta(\boldsymbol{\theta})_r} 
\end{align}

To estimate the parameters of the distribution using some i.i.d. samples $\lbrace x_1, \dots, x_n \rbrace$, we maximise the likelihood of these data under the probability distribution by varying the parameters, whatever parameter values maximise the likelihood are our estimates for the parameters. To do this, we take the derivatives of the likelihood with respect to each of the natural parameters and set this derivative to zero. In practice, we use the log-likelihood. 

The likelihood takes the form 
\begin{align}\label{eq:exponential_family_likelihood}
  P(\lbrace x_1, \dots, x_n \rbrace | \boldsymbol{\theta}) = \prod_{i=1}^n h(x_i) \exp \left( \boldsymbol{\eta}(\boldsymbol{\theta})\cdot \mathbf{T}(x_i) - A(\boldsymbol{\theta}) \right)
\end{align}
The log-likelihood takes the form
\begin{align}
  l(X) = \sum_{i=1}^n \log h(x_i) + \boldsymbol{\eta}(\boldsymbol{\theta})\cdot \mathbf{T}(x_i) - A(\boldsymbol{\theta})
\end{align}
Therefore the derivative takes the form 
\begin{align}
  \pdiff{l(X)}{\eta (\boldsymbol{\theta})_r} = \sum_{i=1}^n \left[ \mathbf{T}(x_i) \right]_r - \pdiff{A(\boldsymbol{\theta})}{\eta(\boldsymbol{\theta})_r}
\end{align}
Setting this expression equal to $0$ gives 
\begin{align}\label{eq:maximum_likelihood_requirement}
  \sum_{i=1}^n \left[ \mathbf{T}(x_i) \right]_r = n\pdiff{A(\boldsymbol{\theta})}{\eta(\boldsymbol{\theta})_r} = n E_P \left[ [\mathbf{T}(x)]_r \right]
\end{align}
So, if we cannot solve the left hand equations in \ref{eq:maximum_likelihood_requirement}, we can use the equity of the left and right expressions in \ref{eq:maximum_likelihood_requirement} as requirements in numerically estimating the parameters.

\subsection{Bayesian parameter estimation}
Every member of the exponential family has a \textit{conjugate prior}. Given some likelihood function, a conjugate prior distribution to that likelihood is one that results in a posterior distribution of the same form as that prior distribution when used in Bayes's Rule. The conjugate prior for a probability distribution  in the form of \ref{eq:exponential_family_definition} has a has the form 
\begin{align}\label{eq:exponential_family_prior}
  P(\boldsymbol{\eta} | \boldsymbol{\chi}, \nu) = f(\boldsymbol{\chi}, \nu)\exp (\boldsymbol{\eta} \cdot \boldsymbol{\chi} - \nu A(\boldsymbol{\eta}))
\end{align}
where $\boldsymbol{\eta}$ represent the natural parameters of the likelihood. Then $\boldsymbol{\chi}$ and $\nu$ are hyperparameters, $f(\boldsymbol{\chi}, \nu)$ is a normalising function, and $A(\boldsymbol{\eta})$ is the same partition function as in \ref{eq:exponential_family_definition}. Note that $\nu > 0$ and $\boldsymbol{\chi} \in \mathbb{R}^s$ where $s$ is the dimensionality of $\boldsymbol{\eta}$ (the number of parameters). $\nu$ controls the effective number of observations contributed by the prior. $\boldsymbol{\chi}$ controls the total amount that these pseudo-observations contribute to the sufficient statistic over all observations and pseudo-observations. Note also that $f$ is defined exactly by the the other functions.

If equation \ref{eq:exponential_family_likelihood} is our likelihood and equation \ref{eq:exponential_family_prior} is our prior, then our posterior takes the form
\begin{align}
  P(\boldsymbol{\eta} | \mathbf{X}, \boldsymbol{\chi}, \nu) & = P(X | \boldsymbol{\eta}) P(\boldsymbol{\eta} | \boldsymbol{\chi}, \nu) \\
  & = \left( \prod_{i=1}^n h(x_i) \exp \left( \boldsymbol{\eta}\cdot \mathbf{T}(x_i) - A(\boldsymbol{\eta}) \right) \right) f(\boldsymbol{\chi}, \nu)\exp (\boldsymbol{\eta} \cdot \boldsymbol{\chi} - \nu A(\boldsymbol{\eta})) \\ 
  & = \left( f(\boldsymbol{\chi}, \nu) \prod_{i=1}^n h(x_i) \right) \exp \left( \sum_{i=1}^n \boldsymbol{\eta} \cdot \mathbf{T}(x_i) - nA(\boldsymbol{\eta}) + \boldsymbol{\eta} \cdot \boldsymbol{\chi} - \nu A(\boldsymbol{\eta}) \right) \\
  & = \left( f(\boldsymbol{\chi}, \nu) \prod_{i=1}^n h(x_i) \right) \exp \left( \boldsymbol{\eta} \cdot \left(\boldsymbol{\chi} + \sum_{i=1}^n\mathbf{T}(x_i) \right) - (n + \nu) A(\boldsymbol{\eta}) \right)
\end{align}
This is the same form as the prior. We can write the posterior as 
\begin{align}
  P(\boldsymbol{\eta} | \mathbf{X}, \boldsymbol{\chi}, \nu) = P (\boldsymbol{\eta} | \boldsymbol{\chi} + \sum_{i=1}^n\mathbf{T}(x_i) , n + \nu )
\end{align}
The maximum a priori estimate for the parameters $\boldsymbol{\eta}$ is the mode of this posterior distribution.

\section{Normal binomial distribution}\label{normal}
\subsection{Probability mass function}
The probability mass function (p.m.f.) for a random variable $X$ with a `normal' binomial distribution with probability of success $p$, and $m$ trials is
\begin{align}
  P(X=k) = \binom{m}{k} p^k (1-p)^{m-k}
\end{align}
that is $k$ successes and $m-k$ failures with a coefficient for the number of different ways we can have $k$ successes and $m-k$ failures. 

\subsection{Likelihood function}
If we take $n$ i.i.d. samples from $X$ consisting of $n$ integers between $0$ and $m$, $\lbrace k_1, \dots , k_n \rbrace$, the probability of these data, aka the likelihood is
\begin{align}
  P(\lbrace k_1, \dots , k_n \rbrace | p) = L(K|p, m) = \prod_{i=1}^n \binom{m}{k_i} p^{k_i}(1-p)^{m-k_i}
\end{align}
that is the product of all the samples.

\subsection{Maximum likelihood estimation}
In order to estimate the value of $p$ from the sample $\lbrace k_1, \dots , k_n \rbrace$, we maximise the value of the likelihood function with respect to $p$. Since $\log$ is an increasing function, maximising the log of the likelihood function is equivalent to maximising the likelihood function. But because likelihood functions tend to take the form of large products, maximising the log of the likelihood is often easier than maximising the likelihood. So we will maximise the log of the likelihood function, a.k.a. the `log likelihood'.
\begin{align}
  l(p, m)  &= \log L(K|p, m) = \log \left( \prod_{i=1}^n \binom{m}{k_i} p^{k_i}(1-p)^{m-k_i} \right) \\
        &= \sum_{i=1}^n \log \binom{m}{k_i} +  k_i \log p + (m - k_i) \log (1-p) 
\end{align}

The derivative with respect to $p$ is
\begin{align}
  \pdiff{l(p, m)}{p} = \sum_{i=1}^n \frac{k_i}{p} - \frac{m - k_i}{1-p}
\end{align}

Letting the derivative equal $0$ gives
\begin{align}
  & \sum_{i=1}^n \frac{k_i}{\hat{p}} - \frac{m - k_i}{1-\hat{p}} = 0 \\
  \implies & \sum_{i=1}^n \frac{k_i (1-\hat{p}) - \hat{p}(m - k_i)}{\hat{p}(1-\hat{p})} = 0 \\
  \implies & \sum_{i=1}^n k_i - k_i \hat{p} - m \hat{p} + k_i \hat{p} = 0 \\
  \implies & \hat{p} = \frac{1}{nm} \sum_{i=1}^n k_i
\end{align}

As you might expect, our maximum likelihood estimate for $p$ is the total number of successes divided by the total number of trials. It's interesting to note that $m$ is technically a parameter of the binomial distribution, and if we didn't know $m$ beforehand, we would be unable to estimate $p$ or $m$ using the maximum likelihood method (I think).

The number of trials $m$ is also a parameter of the binomial distribution. In order to find an estimate for $m$ using maximum likelihood estimation, we take the derivative of $l(p, m)$ with respect to $m$.
\begin{align}
  \pdiff{l(p, m)}{m} = \sum_{i=1}^n \pdiff{\log{\binom{m}{k_i}}}{m} + \log(1-p)
\end{align}

Using 
\begin{align}
  \pdiff{\log \binom{m}{k_i}}{m} = H_m - H_{m - k_i}
\end{align}
where $H_n$ is the $n$th Harmonic number \footnote{\url{https://en.wikipedia.org/wiki/Harmonic_number}}, and letting the derivative equal 0 gives 
\begin{align}
   & \sum_{i=1}^n H_m - H_{m - k_i} + \log (1-p) = 0 \\ 
\end{align}
Theoretically we could sub in our expression for $\hat{p}$ and solve for $m$ (??? is this true ???). But I don't know how to do this right now.

\subsection{Bayesian parameter estimation}
If we consider $m$ to be known and fixed, then the binomial distribution is an exponential family member.
\begin{align}
  P(k | p) & = \binom{m}{k} p^{k}(1-p)^{m-k} \\
  & = \binom{m}{k} \exp \left( k \log p + (m-k) \log (1-p) \right) \\
  & = \binom{m}{k} \exp \left( k \log \frac{p}{1-p} + m \log (1-p) \right)
\end{align}
this takes the form of equation \ref{eq:exponential_family_definition} where $h(x) = \binom{m}{k}$, $\boldsymbol{\eta}(\boldsymbol{\theta}) = \log p/(1-p)$, $\mathbf{T}(k) = k$, and $A(\boldsymbol{\theta}) = -m \log 1-p$. 

This means the conjugate prior for the binomial distribution takes the form
\begin{align}
  P(p | \chi, \nu) = f(\chi, \nu) \exp \left( \chi \log \frac{p}{1-p} - \nu m \log (1 - p)  \right)
\end{align}
or in natural parameter form
\begin{align}
  P(\eta | \chi, \nu) = f(\chi, \nu) \exp \left( \chi \eta + \nu m \log (1 + e^{\eta}) \right)
\end{align}
which gives a posterior of the form
\begin{align}
  P(\eta | \mathbf{X}, \chi, \nu) & = \left( \prod_{i=1}^n \binom{m}{k_i} \exp \left( \eta k_i  + m \log (1 + e^{\eta})  \right) \right) f(\chi, \nu) \exp \left( \chi \eta + \nu m \log (1 + e^{\eta})  \right) \\
  & = \left( f(\chi, \nu) \prod_{i=1}^n \binom{m}{k_i} \right) \exp \left( \eta (\chi + \sum_{i=1}^{n} k_i) + m \log ( 1 + e^{\eta})(n + \nu) \right) \\
  & \propto \exp \left( \eta (\chi + \sum_{i=1}^{n} k_i) + m \log ( 1 + e^{\eta})(n + \nu) \right)
\end{align}
In order to estimate $p$ for our distribution, we pick values for $\chi$ and $\nu$, we find $\hat{\eta} = \argmax_{\eta} P(\eta | \mathbf{X}, \chi, \nu)$, then $\hat{p} = e^{\hat{\eta}}/(1 + e^{\hat{\eta}})$.


Now we know how to do maximum likelihood estimation, we move onto a more complicated distribution.

\section{Conway-Maxwell binomial distribution}
The Conway-Maxwell binomial distribution is similar to the binomial distribution in that we can think of it as a sum of Bernoulli trials. But for the Conway-Maxwell binomial distribution, the Bernoulli variables are associated with each other. Not dependent, not correlated, but associated. 

\subsection{Probability mass function}
The p.m.f. of a random variable $X$ with the Conway-Maxwell binomial distribution with probability of success $p$, dispersion parameter $\nu$, and number of trials $m$ is
\begin{align}
  P(X = k) = \frac{1}{S(p, \nu, m)} \binom{m}{k}^{\nu} p^{k} (1-p)^{m-k}
\end{align}
where 
\begin{align}
  S(p, \nu, m) = \sum_{i=0}^{m} \binom{m}{i}^{\nu} p^{i} (1-p)^{m-i}
\end{align}
is a normalising function. 

The use of the dispersion parameter $\nu$ enables the Conway-Maxwell binomial distribution to `over-disperse' or `under-disperse' relative to a binomial distribution i.e., have greater or lesser variance.

When $\nu > 1$ the distribution is under-dispersed relative to a binomial distribution. In the limit that $\nu \rightarrow \infty$ all the mass accumulates at $m/2$ for even $m$, and at $\lfloor m/2 \rfloor$ and $\lceil m/2 \rceil$ for odd $m$. This corresponds to negatively associated Bernoulli variables. For $\nu < 1$, the distribution is over-dispersed relative to a binomial distribution. In the case where $\nu \rightarrow -\infty$ all the mass is distributed at $0$ and $m$. This is the extreme case of positive association, where all the Bernoulli variables have the value. So, $\nu$ measures the strength of the negative or positive association between the Bernoulli variables that make up the Conway-Maxwell binomial distribution. Note that if $\nu =1$ we have the `normal' binomial distribution described in section \ref{normal}.

\subsection{Likelihood function}
If we take $n$ i.i.d. samples from a random variable $X$ with the Conway-Maxwell binomial distribution with parameters $p$, $\nu$, and $m$ this gives us $n$ integers between $0$ and $m$, $\lbrace k_1, \dots , k_n \rbrace$. These probability or 'likelihood' of these data is
\begin{align}
  P(\lbrace k_1, \dots, k_n \rbrace | p, \nu, m) &= L(K | p, \nu, m) = \prod_{i=1}^{n} \frac{\binom{m}{k_i}^{\nu} p^{k_i} (1-p)^{m-k_i}}{S(p, \nu, m)} \\ 
  & = \frac{\prod_{i=1}^{n} \binom{m}{k_i}^{\nu} p^{k_i} (1-p)^{m-k_i}}{S(p, \nu, m)^n}
\end{align}

Again, just the product of the individual probabilities of each sample.

\subsubsection{Exponential family form}
The likelihood function for a single data point sampled from the Conway-Maxwell binomial distributed variable can be rewritten as 
\begin{align}
  P(k | p, \nu, m) & = \frac{\binom{m}{k}^{\nu} p^{k} (1-p)^{m-k}}{S(p, \nu, m)} \\
  & = \frac{m!^{\nu}}{S(p, \nu, m)} \frac{(1-p)^{m}}{k!^{\nu}(m - k)!^{\nu}} \left( \frac{p}{1-p} \right)^{k} \\
  & = \exp \large( k \log \frac{p}{1-p} - \nu \log(k!(m-k)!) \\ 
  & + \nu \log (m!)  + m \log (1 - p) - \log S(p, \nu, m) \large)
\end{align}

If we let $\pi = \log \frac{p}{1-p}$, then we have 
\begin{align}
  P(k | p, \nu, m) & = \exp \left( k \pi - \nu \log(k!(m-k)!) - m \log (1 + e^{\pi}) + \nu \log m!  - \log S(\pi, \nu, m) \right) \\ 
  & = h (k) \exp \left( \boldsymbol{\theta} \cdot \mathbf{T} (x) - A (\boldsymbol{\theta}) \right)
\end{align}
where $\boldsymbol{\theta} = (\pi, -\nu)$, $h(k) = 1$, $A(\boldsymbol{\theta}) = m \log (1 + e^{\pi}) - \nu \log m!  + \log S(\pi, \nu, m)$ and $\mathbf{T} (x) = (k, \log(k!(m-k)!)$.
When we consider $m$ to be fixed and known, $\boldsymbol{\theta}$ are the natural parameters of the distribution, and $\mathbf{T} (x)$ are the natural sufficient statistics.

\subsection{Maximum likelihood estimation}
Since the natural parameters of the Conway-Maxwell binomial distribution are $\left( \log \frac{p}{1-p}, -\nu \right)$ and the natural sufficient statistics are $\left( \sum_{i=1}^n k_i, \sum_{i=1}^n \log (k! (m-k)!) \right)$, the values of $p$ and $\nu$ satisfy 
\begin{align}
  E_{P(p,\nu)}[k] & = \pdiff{A(\boldsymbol{\theta})}{\pi} \\
  E_{P(p, \nu)}[\log (k!(m-k)!)] & = \pdiff{A(\boldsymbol{\theta})}{\nu}
\end{align}
at the point of maximum likelihood.

Once again, we maximise the log-likelihood function, which is
\begin{align}
  l(p, \nu, m) &= \log L(K | p, \nu, m) = \log \frac{\prod_{i=1}^{n} \binom{m}{k_i}^{\nu} p^{k_i} (1-p)^{m-k_i}}{S(p, \nu, m)^n} \\
  &= - n \log S(p, \nu, m) + \sum_{i=1}^{n} \log \binom{m}{k_i}^{\nu} + k_i \log p + (m - k_i) \log (1-p)
\end{align}

Maximising will involve calculating the partial derivative of $\log S(p, \nu, m)$ with respect to the parameters. 

\begin{align}
  \pdiff{\log S(p, \nu, m)}{p} = \frac{1}{S(p, \nu, m)} \pdiff{S(p, \nu, m)}{p}
\end{align}

For the partial derivative without the $\log$, we have
\begin{align}
  \pdiff{S(p, \nu, m)}{p} &= \pdiff{\sum_{i=0}^m \binom{m}{i}^{\nu} p^i (1-p)^{m-i}}{p} \\
  & = \sum_{i=0}^m \binom{m}{i}^{\nu} \left[ i p^{i-1}(1-p)^{m-i} - (m-i)p^i(1-p)^{m-i-1} \right]
\end{align}
which gives
\begin{align}
  \pdiff{\log S(p, \nu, m)}{p} & = \sum_{i=0}^m \frac{\binom{m}{i}^{\nu} \left[ i p^{i-1}(1-p)^{m-i} - (m-i)p^i(1-p)^{m-i-1} \right]}{\binom{m}{i}^{\nu} p^{i} (1-p)^{m-i}} \\
  & = \sum_{i=0}^m i p^{-1} - (m-i) (1-p)^{-1} \\
  & = \sum_{i=0}^m \frac{i}{p} - \frac{m-i}{1-p} \\
  & = \sum_{i=0}^m \frac{i(1-p) - (m-i)p}{p(1-p)} \\
  & = \sum_{i=0}^m \frac{i - mp}{p(1-p)} \\
  & = \frac{m(m+1) -2m(m+1)p}{2p(1-p)} \\
  & = \frac{m(m+1)(1-2p)}{2p(1-p)}
\end{align}
using $\sum_{i=0}^{m} i = m(m+1)/2$.

So the partial derivative of the log-likelihood function with respect to $p$ is
\begin{align}
  \pdiff{l(p,\nu,m)}{p} & = -n \pdiff{\log S(p, \nu, m)}{p} + \sum_{i=1}^n \frac{k_i}{p} - \frac{m-k_i}{1-p} \\
  & = -n \pdiff{\log S(p, \nu, m)}{p} + \sum_{i=1}^n \frac{k_i - mp}{p(1-p)} \\
  & = \frac{-nm(m+1)(1-2p)}{2p(1-p)} + \sum_{i=1}^n \frac{k_i - mp}{p(1-p)} \\
  & = \frac{-nm(m+1)(1-2p) - 2nmp + 2\sum_{i=1}^n k_i}{2p(1-p)} \\
  & = \frac{-nm\left[ (m+1)(1-2p) + 2p \right] + 2\sum_{i=1}^n k_i}{2p(1-p)} \\
  & = \frac{-nm \left[m - 2mp + 1 -2p +2p \right] + 2 \sum_{i=1}^n k_i}{2p(1-p)} \\
  & = \frac{-nm\left[ m(1-2p) + 1\right] + 2 \sum_{i=1}^n k_i}{2p(1-p)}
\end{align}

\newpage
\bibliography{binomial_maximum_likelihood.bbl}

\end{document}
