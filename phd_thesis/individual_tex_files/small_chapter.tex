\documentclass[a4paper,12pt]{article}
\usepackage[utf8x]{inputenc}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{mathrsfs}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage[margin=2cm]{geometry}
\usepackage{times}
\usepackage{graphicx}
\usepackage{enumitem}
\usepackage{fancyhdr}
\usepackage{hyperref}
\usepackage{setspace}
\usepackage{subcaption}
\usepackage{mathtools}

% stuff added by Cian
\usepackage{lineno}
\linenumbers
\renewcommand{\baselinestretch}{1.5}
\usepackage{authblk}

% biliography stuff
%\usepackage[
%backend=biber,
%style=alphabetic,
%sorting=ynt
%]{biblatex}

%\addbibresource{calcium_imaging_model_refs.bib}

\newtheorem{theorem}{Theorem}
\newtheorem{proposition}{Proposition}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{corollary}{Corollary}[section]
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]

\newcommand{\boldnabla}{\mbox{\boldmath$\nabla$}} % to be used in mathmode
\newcommand{\cbar}{\overline{\mathbb{C}}}% to be used in mathmode
\newcommand{\diff}[2]{\frac{d #1}{d #2}}% to be used in mathmode
\newcommand{\difff}[2]{\frac{d^2 #1}{d #2^2}}% to be used in mathmode
\newcommand{\pdiff}[2]{\frac{\partial #1}{\partial #2}} % to be used in mathmode
\newcommand{\pdifff}[2]{\frac{\partial^2 #1}{\partial #2^2}}% to be used in mathmode
\newcommand{\upperth}{$^{\mbox{\footnotesize{th}}}$}%to be used in text mode
\newcommand{\vect}[1]{\mathbf{#1}}% to be used in mathmode
\newcommand{\curl}[1]{\boldnabla \times \vect{#1}} % to be used in mathmode
\newcommand{\divr}[1]{\boldnabla \cdot \vect{#1}} %to be used in mathmode
\newcommand{\modu}[1]{\left| #1 \right|} %to be used in mathmode
\newcommand{\brak}[1]{\left( #1 \right)} % to be used in mathmode
\newcommand{\comm}[2]{\left[ #1 , #2 \right]} %to be used in mathmode
\newcommand{\dop}{\vect{d}} %to be used in mathmode
\newcommand{\cov}{\text{cov}} %to be used in mathmode
\newcommand{\var}{\text{var}} %to be used in mathmode
\newcommand{\mb}{\mathbf} %to be used in mathmode
\newcommand{\bs}{\boldsymbol} %to be used in mathmode
% Title Page
\title{Studies with practical limitations \& negative results}
\date{}

\author[1]{Thomas J. Delaney}
\affil[1]{School of Computer Science, Electrical and Electronic Engineering, and Engineering Mathematics, University of Bristol, Bristol, United Kingdom.}
\renewcommand\Affilfont{\itshape\small}

\begin{document}

\maketitle

\abstract{Here I will present some details on research topics that I started, but that unfortunately did not lead anywhere useful. There are two pieces of research, based on two papers. Each paper is related to the overall theme of my PhD of analysing and modelling behaviours of populations of neurons. The first part is based on a model of parallel spike trains including higher order interactions by Shimazaki et al (2012). The second part is based on a multiscale model for making inferences on hierarchical data.}

\section{Dynamic state space model of pairwise and higher order neuronal correlations}
In their paper Shimazaki et al (2012) aimed to model spike trains from populations of neurons in parallel, with pairwise and higher order dynamic interactions between the trains. They modelled the spike trains as multi-variate binary processes using a log-linear model, and they extracted spike interaction parameters using a Bayesian filter/EM-algorithm. They developed a goodness-of-fit measure for the model to test if including these higher order correlations is necessary for an accurate model. Their measure was based on the Bayes factor but they also assessed the suitability of higher order models using the AIC and BIC. So the increase in the number of parameters associated with fitting higher order interactions was taken into account. They tested the performance of the model on synthetic data with known higher order correlations. They used the model to look for higher order correlations in data from awake behaving animals. They use the model to demonstrate dynamic appearance of higher order correlations in the monkey motor cortex\cite{shimazaki}.

We used the available Python repository to implement the model, and we successfully worked through the tutorial provided. But we found that the model did not scale well to larger populations. We attempted to fit the model to a population of $10$ neurons and found we didn't manage to finish the process. Since, our goal was to find a model to scale to hundreds or thousands of neurons, we decided that this model was no longer worth pursuing.

\section{A multiscale model for hierarchical data applied to \\ neuronal data}
In their paper Kolacayk et al (2001) developed a framework for a modelling hierarchically aggregated data, and making inferences based on a model arising from this framework. They assumed that a hierarchical aggregation existed on the data in question, where each element at each level of the hierarchy had some associated measurements, an associated mean process, which was the expected value of these measurements. They also assumed that the measurements of each parent were equal to the sum of the measurements from all of its children. They showed that these assumptions gave rise to a relationship between parent and child measurements across all levels of the hierarchy, where the product of the likelihood of the parameters of the lowest level of the hierarchy can be expressed as products of conditional likelihoods of the elements of higher levels of the hierarchy\cite{kolacayk}.

They gave examples of these expressions for measurements sampled from Gaussian distributions, and Poisson distributions, and showed the definitions of the hierarchical parameters which reparametrise the distribution of these data taking the hierarchy into account. They go on to suggest prior distributions for this multiscale model, and integrate these priors to give posterior distributions for the measurements from each element at each level in the hierarchy, and expressions for the MAP estimated parameters of each the associated processes\cite{kolacayk}.

We implemented their model in Python by creating some synthetic data from Poisson distributions, and defining a hierarchy by agglomerating these data. We calculated the MAP estimates using our knowledge of the hierarchy, and using the expressions given in the paper. We found that the MAP estimates were far less accurate than would be achieved by simply ignoring the hierarchy during estimation, and using a maximum likelihood approach. After that, we decided to move on.

\bibliography{small_chapter.bbl}

\end{document}
